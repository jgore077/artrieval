# artrieval
This repo houses our experiments with cross-modal artwork retrieval.

## installation
Prepare environment for [CLIP](https://github.com/openai/CLIP)  
Download [LongCLIP-L](https://huggingface.co/BeichenZhang/LongCLIP-L) checkpoint  
Place the checkpoint.pt file in long_clip/checkpoints

## documentation

**Folders:**  
./data:  
- Current storage for all our data generated within our code

./long_clip:  
- Contains the Long-CLIP model, obtained from the Long-CLIP creators [repo](https://github.com/beichenzbc/Long-CLIP)

./metaprocessor:  
- Module that contains classes and functions used for processing our dataset

./plots:  
- Storage for plots generated during evaluation of the quality of our dataset

./scripts:
- Scripts used to call individual functions to modify/evaluate our dataset. Note: The functions can be directly called from a pipeline, like meta_processing.py, instead of using this scripts folder.

**Individual files:**  
bins.json - Our dataset after some preprocessing. This is not the finalized version of the dataset, and the whole pre-processing pipeline was not used on it.  
eval.py - Used for running a Long-CLIP model on our dataset and obtaining evaluation metrics.  
fine-tune.py - The fine-tuning code for Long-CLIP, see [reference](https://github.com/zer0int/Long-CLIP/blob/main/ft-B-train-LongCLIP-ViT-L-14.py).  
format.py - Format our dataset to work as input to Long-CLIP.  
generate_bins.py - The pipeline used for generating "bins.json".
meta_processing.py - Another processing pipeline example, full processing pipeline will be added here soon.  
predictions.json - Classification probabilities for sentences of each description. Generated by visualContextualBins function in /metaprocessor/mutations.py.  
requirements.txt - Requirements needed for this repo, not currently up-to-date.  
run.py - Example code for running Long-CLIP, we might replace this with eval.py.
